{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.1-cp310-cp310-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp310-cp310-win_amd64.whl.metadata (103 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in d:\\pyspark\\ps\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\pyspark\\ps\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.1.0-cp310-cp310-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\pyspark\\ps\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\pyspark\\ps\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.10.1-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 4.7/8.1 MB 23.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 20.0 MB/s eta 0:00:00\n",
      "Using cached contourpy-1.3.1-cp310-cp310-win_amd64.whl (218 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp310-cp310-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 20.6 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.8-cp310-cp310-win_amd64.whl (71 kB)\n",
      "Downloading pillow-11.1.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.6/2.6 MB 15.1 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pillow-11.1.0 pyparsing-3.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk \n",
    "import gensim\n",
    "import string \n",
    "import matplotlib.pyplot as plt \n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"My name is sumanth\",\n",
    "    \"i use to teach all the data stack along with the ops and cloud\",\n",
    "    \"nlp is very amazing\",\n",
    "    \"we are trying to learn word2vec\",\n",
    "    \"we are trying to build two models word 2 vec that is cbow and skipgrams\",\n",
    "    \"nlp is part of ai \",\n",
    "    \"my phone number is 88383883853678475\",\n",
    "    \"word2vec is being used for word embedding\",\n",
    "    \"It is going to perform better than one hot encoding , bow and tfidf\"\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"my name is sumanth 123433 . \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my name is sumanth 123433 . '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my name is sumanth  . '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"\\d+\",\"\",s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my name is sumanth 123433  '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.translate(str.maketrans(\"\",\"\",string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'name', 'is', 'sumanth', '123433', '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = ['my', 'name', 'is', 'sumanth', '123433', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'sumanth', '123433', '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in s1 if i not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_processing(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d+\",'',text)\n",
    "    text = text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    word = word_tokenize(text)\n",
    "    word = [i for i in word if i not in stopwords.words('english')]\n",
    "    return text \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi hw r u '"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_processing(\"hi hw r u \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My name is sumanth',\n",
       " 'i use to teach all the data stack along with the ops and cloud',\n",
       " 'nlp is very amazing',\n",
       " 'we are trying to learn word2vec',\n",
       " 'we are trying to build two models word 2 vec that is cbow and skipgrams',\n",
       " 'nlp is part of ai ',\n",
       " 'my phone number is 88383883853678475',\n",
       " 'word2vec is being used for word embedding',\n",
       " 'It is going to perform better than one hot encoding , bow and tfidf']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus = [word_processing(sentence )for sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my name is sumanth',\n",
       " 'i use to teach all the data stack along with the ops and cloud',\n",
       " 'nlp is very amazing',\n",
       " 'we are trying to learn wordvec',\n",
       " 'we are trying to build two models word  vec that is cbow and skipgrams',\n",
       " 'nlp is part of ai ',\n",
       " 'my phone number is ',\n",
       " 'wordvec is being used for word embedding',\n",
       " 'it is going to perform better than one hot encoding  bow and tfidf']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window = is the parameter keeps the number of features to predict the output # more the window more the context or semantic meaning in the sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_cbow = Word2Vec(sentences=processed_corpus,vector_size=100,window=5,min_count = 1,sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_skipgram = Word2Vec(sentences=processed_corpus,vector_size=100,window=5,min_count = 1,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_count is the paramater like the number of times every word should be considered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cbow = target word prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipgrams = target word is given and predicting the input words  (surroundingwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_cbow.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x249dbfc4b50>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.9684329e-03,  6.5288129e-03,  2.8575112e-03,  1.1406825e-02,\n",
       "        5.8775968e-05, -7.4333875e-03,  1.2916059e-04,  4.3849349e-03,\n",
       "       -5.6981393e-03, -1.0432433e-02, -5.2236360e-03, -9.1367848e-03,\n",
       "        4.9539353e-03,  3.9592069e-03,  8.9971395e-03,  6.4365370e-03,\n",
       "       -4.5932308e-03,  3.6751372e-03,  7.1641728e-03, -1.6233566e-03,\n",
       "        4.1388860e-03,  7.3473724e-03, -5.9296298e-03,  9.4238790e-03,\n",
       "        6.8782056e-03,  9.8911542e-03, -9.9398065e-03,  5.8134687e-03,\n",
       "        6.5836557e-03, -1.0156713e-03, -4.9621137e-03,  3.5237996e-03,\n",
       "       -5.9982189e-03, -7.3800734e-03,  2.7311053e-03,  9.5716109e-03,\n",
       "        1.0503758e-02, -3.1823036e-03, -9.6120555e-03,  9.1512706e-03,\n",
       "        8.6020306e-03, -5.8140010e-03, -9.8672705e-03, -3.6377355e-03,\n",
       "       -3.5346900e-03, -3.3141219e-03, -1.7613750e-03, -7.3445454e-04,\n",
       "       -1.9515639e-03, -5.8993641e-03,  9.6841594e-03, -6.9492813e-03,\n",
       "       -7.5905202e-03,  8.2132984e-03,  8.1808516e-04,  9.2931045e-03,\n",
       "       -1.7145297e-03, -8.3409362e-03,  4.0509275e-04, -4.1550596e-04,\n",
       "       -9.5328987e-03, -1.3660077e-03,  5.9395572e-03,  6.6302582e-03,\n",
       "        7.2342632e-03,  5.6454246e-03, -4.2663803e-03,  6.2070636e-04,\n",
       "       -5.2203154e-03,  4.9931807e-03, -6.3427314e-03,  3.1957093e-03,\n",
       "        1.7048823e-04, -4.8407172e-03, -5.5900011e-03,  6.9306651e-03,\n",
       "       -8.0550900e-03,  2.9663387e-04,  2.2840200e-03,  6.6412571e-03,\n",
       "       -9.1201924e-03,  1.8353402e-03,  1.1453866e-02, -6.9057876e-03,\n",
       "       -3.8640979e-03,  4.8837513e-03,  7.2578769e-03,  1.5372422e-03,\n",
       "        7.9836994e-03,  9.6649217e-04,  3.8655633e-03,  3.5776600e-04,\n",
       "       -8.2836673e-03,  3.1524224e-03,  4.5183795e-03, -7.5478363e-03,\n",
       "        7.1114162e-04, -1.0087506e-03, -1.8710675e-04,  1.0539794e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_skipgram.wv['k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_skipgram.wv.similarity(\"e\",\"e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_skipgram.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t', 0.646964967250824),\n",
       " ('o', 0.6185289621353149),\n",
       " ('l', 0.6088563203811646),\n",
       " ('c', 0.6036065220832825),\n",
       " (' ', 0.5873371362686157),\n",
       " ('a', 0.5287965536117554),\n",
       " ('f', 0.5260787606239319),\n",
       " ('r', 0.5259928703308105),\n",
       " ('h', 0.5210593342781067),\n",
       " ('u', 0.5196312069892883)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_skipgram.wv.most_similar('e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'e',\n",
       " 'o',\n",
       " 't',\n",
       " 'n',\n",
       " 'a',\n",
       " 'i',\n",
       " 'r',\n",
       " 'd',\n",
       " 's',\n",
       " 'm',\n",
       " 'g',\n",
       " 'w',\n",
       " 'h',\n",
       " 'l',\n",
       " 'c',\n",
       " 'b',\n",
       " 'p',\n",
       " 'u',\n",
       " 'y',\n",
       " 'f',\n",
       " 'v',\n",
       " 'k',\n",
       " 'z']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_skipgram.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('u', 0.23937146365642548),\n",
       " ('l', 0.2384229600429535),\n",
       " ('h', 0.2332485318183899),\n",
       " ('k', 0.2205420434474945),\n",
       " ('c', 0.19928202033042908)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_cbow.wv.most_similar(\"e\",topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t', 0.646964967250824),\n",
       " ('o', 0.6185289621353149),\n",
       " ('l', 0.6088563203811646),\n",
       " ('c', 0.6036065220832825),\n",
       " (' ', 0.5873371362686157)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_skipgram.wv.most_similar(\"e\",topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = word2vec_cbow.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = word_vector.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'e',\n",
       " 'o',\n",
       " 't',\n",
       " 'n',\n",
       " 'a',\n",
       " 'i',\n",
       " 'r',\n",
       " 'd',\n",
       " 's',\n",
       " 'm',\n",
       " 'g',\n",
       " 'w',\n",
       " 'h',\n",
       " 'l',\n",
       " 'c',\n",
       " 'b',\n",
       " 'p',\n",
       " 'u',\n",
       " 'y',\n",
       " 'f',\n",
       " 'v',\n",
       " 'k',\n",
       " 'z']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([word_vector[i] for i in vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = tsne.fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(x_new[:,0],x[:,1],marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i ,words in enumerate(vocab[:len(x_new)]):\n",
    "    plt.annotate(words,xy=(x_new[i,0],x_new[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
